# ğŸ¥ HealthCare Data Pipeline Project with PySpark

This project implements a **production-grade ETL pipeline** for a Healthcare system using **PySpark on Databricks**. It processes both static and daily datasets, applies **schema enforcement**, ensures **data quality**, and models data into a **star schema** for analytics.

---

## ğŸ¯ Project Objective

The goal is to simulate a **daily batch pipeline** for a healthcare provider that ingests, transforms, and stores clinical and claims data for analytical consumption.

### Key Objectives:

- ğŸ“… **Daily ingestion** of claim and patient files with `batch_date` in filenames  
- ğŸ›¡ï¸ **Data validation** for null checks and schema enforcement  
- ğŸ” **SCD Type 2 handling** for patient dimension  
- ğŸ§¹ **Quarantining** of bad records  
- ğŸ§¼ **Deduplication** of all entities  
- ğŸ—ƒï¸ Generate **star schema** outputs: fact and dimension tables  
- â˜ï¸ Simulated **S3 output using DBFS**  

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ HealthCare_Project.py   # Main PySpark ETL pipeline
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ HLD/
â””â”€â”€ sample_data/
```

---

## ğŸ—ï¸ Architecture Overview

**Tech Stack:**
- âš™ï¸ PySpark (Databricks)
- ğŸ—‚ï¸ DBFS (Simulated S3 storage)
- ğŸ§¬ Star Schema modeling
- ğŸ“Š BI-ready fact and dimension outputs

**Pipeline Flow:**

```
CSV Files (DBFS) 
    â†“
PySpark (Read â†’ Validate â†’ SCD2 Transform)
    â†“
Dim & Fact tables (Saved to DBFS path as CSVs)
```

---

## ğŸ“š Datasets Processed

- `claims_<batch_date>.csv` â€“ daily healthcare claims  
- `patient_<batch_date>.csv` â€“ daily updated patient info  
- `provider/version=<date>/provider.csv` â€“ static provider info (versioned)  
- `diagnosis/version=<date>/diagnosis.csv` â€“ static diagnosis codes (versioned)  
- `procedure/version=<date>/procedure.csv` â€“ static procedure codes (versioned)  

---

## ğŸ› ï¸ Features Implemented

âœ… Schema enforcement using PySpark `StructType`  
âœ… Null validation for critical fields  
âœ… Quarantine logic for invalid records (logged)  
âœ… Deduplication of all datasets  
âœ… Dynamic loading of latest versioned static files  
âœ… Full **SCD Type 2 handling** for patient dimension  
âœ… Star schema with fact and dimension outputs  
âœ… CSV writes to partitioned output folder: `/FileStore/tables/healthcare/processed/<batch_date>/`

---

## ğŸ—ƒï¸ Star Schema Tables

### Dimension Tables
- `dim_patients` (SCD2 enabled)
- `dim_provider`
- `dim_diagnosis`
- `dim_procedure`

### Fact Tables
- `fact_claim`

---

## ğŸ§ª Sample BI Queries (Post-Load Use Cases)

```sql
-- Total claims per provider
SELECT provider_id, SUM(claim_amount)
FROM fact_claim
GROUP BY provider_id;

-- Identify frequently claimed procedures
SELECT procedure_code, COUNT(*) AS claim_count
FROM fact_claim
GROUP BY procedure_code
ORDER BY claim_count DESC;

-- Current active patient profiles
SELECT * FROM dim_patients
WHERE is_current = true;
```

---

## ğŸš€ Future Enhancements

- Integrate with **Airflow for scheduling**
- Implement **Delta Lake** and data versioning
- Add **unit tests** for schema and quality logic
- Implement full **audit logging and lineage**

---

## ğŸ“‚ How to Run

1. Place input datasets under `/FileStore/tables/healthcare/`
2. Set the desired `batch_date` in the script (default: `2025_06_18`)
3. Run `HealthCare_Project.py` in Databricks
4. Output files will be saved to `/FileStore/tables/healthcare/processed/<batch_date>/`
5. Use Spark or external BI tools to query final outputs

---

## ğŸ‘¨â€ğŸ’» Author

**Pratyush Sahay**  
pratyush(dot)310899@gmail(dot)com  
Data Engineer | 3 Years Experience  
LinkedIn: [www.linkedin.com/in/pratyush-sahay](https://www.linkedin.com/in/pratyush-sahay)
